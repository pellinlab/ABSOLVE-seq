{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jDB-HAhEY8Da"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import glob\n",
    "import multiprocessing\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "oligo_df = pd.read_excel('../test/data/target_info/LVOTUMIv7_untested_ALT_oligos_sgRNAs.xlsx', sheet_name='OT_oligo_designs_ORIGINAL', skiprows=1).rename(columns={'Unnamed: 1':'Barcode'})\n",
    "oligo_df['OT_sequence'] = oligo_df['~3 upstream, ~20 protospacer, 3 PAM, 6 downstream'].str.upper()\n",
    "oligo_df['OT'] = oligo_df['OT'].str.replace('_','-')\n",
    "oligo_df = oligo_df.set_index('OT')[['Barcode', 'OT_sequence']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "oligo_df.to_csv('../test/data/target_info/target_oligos_sequenes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJDBWDIBY9Vu"
   },
   "outputs": [],
   "source": [
    "# tar xzvf alleles_HiFi_WTCas9_NoEP.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Directory: /Users/jieconglin/Documents/GitHub/ABSOLVE-seq/test/absolveseq_edits\n"
     ]
    }
   ],
   "source": [
    "file_path = '/Users/jieconglin/Documents/GitHub/ABSOLVE-seq/test/absolveseq_edits/raw/'\n",
    "\n",
    "# 获取当前文件的目录\n",
    "current_dir = os.path.dirname(file_path)\n",
    "# 获取上一级目录\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "print(\"Parent Directory:\", parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kk13hBj0Y8Dd"
   },
   "outputs": [],
   "source": [
    "# Functions to process UMI alleles\n",
    "def filter_low_reads_umiallele(df):\n",
    "    filtered_df = df[df['#Reads'] > 1].reset_index()\n",
    "    filtered_df['%Reads'] = filtered_df['#Reads']/filtered_df['#Reads'].sum() * 100\n",
    "    return filtered_df\n",
    "\n",
    "def deduplicate_umiallele(df, filtering=False):\n",
    "    tdf = df.sort_values(by=['UMI', '%Reads_UMI'], ascending=False)\n",
    "    tdf_dedup = tdf.drop_duplicates(subset='UMI', keep='first')\n",
    "    if filtering:\n",
    "        filtered_df = tdf_dedup[tdf_dedup['%Reads_UMI'] > 65].reset_index()\n",
    "    else:\n",
    "        filtered_df = tdf_dedup.reset_index()\n",
    "    filtered_df['#Reads'] = 1\n",
    "    filtered_df['%Reads'] = filtered_df['#Reads']/filtered_df['#Reads'].sum() * 100\n",
    "    return filtered_df\n",
    "\n",
    "def levenshteinFullMatrix(str1, str2):\n",
    "    m = len(str1)\n",
    "    n = len(str2)\n",
    "    # Initialize a matrix to store the edit distances\n",
    "    dp = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "\n",
    "    # Initialize the first row and column with values from 0 to m and 0 to n respectively\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    # Fill the matrix using dynamic programming to compute edit distances\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                # Characters match, no operation needed\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                # Characters don't match, choose minimum cost among insertion, deletion, or substitution\n",
    "                dp[i][j] = 1 + min(dp[i][j - 1], dp[i - 1][j], dp[i - 1][j - 1])\n",
    "\n",
    "    # Return the edit distance between the strings\n",
    "    return dp[m][n]\n",
    "\n",
    "def add_cutsite(fn):\n",
    "    out_name = fn.split('/')[-1]\n",
    "    allele_full_df = pd.read_csv(fn, sep='\\t')\n",
    "    align_20bp = []\n",
    "    ref_20bp = []\n",
    "    for idx, row in allele_full_df.iterrows():\n",
    "        ref_pos = np.array(ast.literal_eval(row['ref_positions']))\n",
    "        cutsite = row['sgRNA_cut_points']\n",
    "        cutsite = np.arange(len(row['Reference_Sequence']))[ref_pos == cutsite][0]\n",
    "        align_20bp.append(row['Aligned_Sequence'][cutsite-9:cutsite+11])\n",
    "        ref_20bp.append(row['Reference_Sequence'][cutsite-9:cutsite+11])\n",
    "    allele_full_df['Unedited'] = allele_full_df['Read_Status'] == 'UNMODIFIED'\n",
    "    allele_full_df['Aligned_Sequence_20bp'] = align_20bp\n",
    "    allele_full_df['Reference_Sequence_20bp'] = ref_20bp\n",
    "    allele_full_df.to_csv(fn, sep='\\t', index=False)\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkcFryrsY8Dd"
   },
   "outputs": [],
   "source": [
    "# Add cutting site to allele table\n",
    "allele_table_list = []\n",
    "allele_table_dir = './alleles_HiFi_WTCas9_NoEP/'\n",
    "for fn in glob.glob(allele_table_dir+'/*_full_withUMI.txt'):\n",
    "    allele_table_list.append(fn)\n",
    "with Pool(processes=10) as pool:\n",
    "    results = []\n",
    "    for result in tqdm(pool.imap_unordered(add_cutsite, allele_table_list), total=len(allele_table_list)):\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwlE0Hf2Y8De"
   },
   "source": [
    "## Load UMI annotation from plasmid pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AW8Q2WhEY8De"
   },
   "outputs": [],
   "source": [
    "maxi_ds_df = pd.read_csv('./plasmid_pools_data/trimmed_LV-Maxi-merged3_L001_R1_001.fastq.gz.extractedSeq.UMI-Barcode-dedup-dedup.annot.csv')\n",
    "maxi_ds_df['good_L0'] = maxi_ds_df['with_expected_OT']\n",
    "maxi_ds_df['good_S'] = (maxi_ds_df['#UMI-Barcode'] == maxi_ds_df['#UMI-Seq-merged'])&(maxi_ds_df['with_expected_OT'])\n",
    "umi_complexity = pd.read_csv('./plasmid_pools_data/UMI_entropy_barcode_count_merged_plasmid_pools.csv', index_col='umi_r1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lEO3It1Y8De"
   },
   "source": [
    "## Annotate Raw allele based on Barcode annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeBlKHCGY8De"
   },
   "outputs": [],
   "source": [
    "os.makedirs('./target_umi_barcode_table_WTCas9_NoEP', exist_ok=True)\n",
    "out_folder = './target_umi_barcode_table_WTCas9_NoEP/raw/'\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "# allele_table_dir = './alleles_HiFi_WTCas9_NoEP/'\n",
    "for fn in tqdm(glob.glob(allele_table_dir + '/*_full_withUMI.txt')):\n",
    "    ot_name = fn.split('/')[-1].split('_')[0]\n",
    "    if ot_name not in sel_OTs:\n",
    "        continue\n",
    "    out_name = fn.split('/')[-1]\n",
    "    allele_df = pd.read_csv(fn, sep='\\t')[['UMI', 'index', 'Aligned_Sequence',\n",
    "               'Reference_Sequence', 'Aligned_Sequence_20bp', 'Reference_Sequence_20bp', 'sgRNA_cut_points', 'Aligned_Reference_Scores', 'Unedited', 'n_deleted', 'n_inserted',\n",
    "               'n_mutated', '#Reads', '%Reads_UMI', '%Reads', 'sample_id', 'donor',\n",
    "               'replicate', 'group', 'OT_name']]\n",
    "    ot_name = allele_df.iloc[0]['OT_name'].replace('-', '_')\n",
    "    OT_maxi_df = maxi_ds_df[(maxi_ds_df['OT'] == ot_name)].drop_duplicates(subset='umi_r1', keep='first')\n",
    "    umi_set = list(set(allele_df['UMI'].unique()).intersection(umi_complexity.index))\n",
    "    allele_df = allele_df.merge(umi_complexity.loc[umi_set][['#barcode', 'entropy']], left_on='UMI', right_index=True, how='left')\n",
    "    allele_maxi_df = allele_df.merge(OT_maxi_df, how='left', left_on='UMI', right_on='umi_r1')\n",
    "    allele_maxi_df.loc[allele_maxi_df[allele_maxi_df['good_L0'].isna()].index, 'good_L0'] = 'Unseen'\n",
    "    allele_maxi_df.loc[allele_maxi_df[allele_maxi_df['with_expected_OT'].isna()].index, 'with_expected_OT'] = 'Unseen'\n",
    "    allele_maxi_df['good_L0'] = allele_maxi_df['good_L0'].astype(str)\n",
    "    # Stringent\n",
    "    allele_maxi_df.loc[allele_maxi_df[allele_maxi_df['good_S'].isna()].index, 'good_S'] = 'Unseen'\n",
    "    allele_maxi_df.loc[allele_maxi_df[allele_maxi_df['with_expected_OT'].isna()].index, 'with_expected_OT'] = 'Unseen'\n",
    "    allele_maxi_df['good_S'] = allele_maxi_df['good_S'].astype(str)\n",
    "    allele_maxi_df['with_expected_OT'] = allele_maxi_df['with_expected_OT'].astype(str)\n",
    "    allele_maxi_df.to_csv(out_folder + '/' + out_name.replace('.txt', '.annot.txt'), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkKawg0hY8De"
   },
   "source": [
    "## Filter alleles based on UMI annotetion (Dedud, Dedup and Dedud+dedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kV_q6dmlY8De"
   },
   "outputs": [],
   "source": [
    "dir_path = './target_umi_barcode_table_WTCas9_NoEP/' # output folder\n",
    "for fn in tqdm(glob.glob(dir_path +'/raw/*.annot.txt')):\n",
    "    out_name = fn.split('/')[-1]\n",
    "    allele_df = pd.read_csv(fn, sep='\\t')\n",
    "    # allele_df['UMI-Aligned_Sequence'] = allele_df['UMI'] + '-' + allele_df['Aligned_Sequence']\n",
    "    donor = allele_df.iloc[0]['donor']\n",
    "    edit = allele_df.iloc[0]['group']\n",
    "    allele_deduplicate_df = deduplicate_umiallele(allele_df)# .drop(columns='level_0')\n",
    "    out_folder = '{}/raw_dedup/'.format(dir_path)\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.mkdir(out_folder)\n",
    "    out_fn = out_folder + out_name\n",
    "    allele_deduplicate_df.to_csv(out_fn, sep='\\t', index=False)\n",
    "\n",
    "    for method in ['S']:\n",
    "        umi_type_name = 'good_' + method\n",
    "        dedud_df = allele_df[allele_df[umi_type_name] == 'True'].reset_index()\n",
    "        dedud_df['%Reads'] = dedud_df['#Reads']/dedud_df['#Reads'].sum() * 100\n",
    "        dedud_df = dedud_df.drop(columns='level_0')\n",
    "        out_folder = dir_path + '/dedud{}/'.format(method)\n",
    "        if not os.path.exists(out_folder):\n",
    "            os.mkdir(out_folder)\n",
    "        dedud_df.to_csv(out_folder + out_name, sep='\\t', index=False)\n",
    "\n",
    "        # allele_deduplicate_df = deduplicate_umiallele(dedud_df).drop(columns='level_0')\n",
    "        # out_folder = './target_umi_barcode_table_full/dedud{}_dedup/'.format(method)\n",
    "        # if not os.path.exists(out_folder):\n",
    "        #     os.mkdir(out_folder)\n",
    "        # out_fn = out_folder + out_name\n",
    "        # allele_deduplicate_df.to_csv(out_fn, sep='\\t')\n",
    "\n",
    "        dedud_filtered_df = dedud_df[(dedud_df['entropy'] > 1.477)&(dedud_df['#barcode'] == 1)|(dedud_df['#barcode'] == 2)]\n",
    "        out_folder = dir_path + '/dedud{}_filtered/'.format(method)\n",
    "        if not os.path.exists(out_folder):\n",
    "            os.mkdir(out_folder)\n",
    "        dedud_filtered_df.to_csv(out_folder + out_name, sep='\\t', index=False)\n",
    "\n",
    "        allele_deduplicate_df = deduplicate_umiallele(dedud_filtered_df).drop(columns='level_0')\n",
    "        out_folder = dir_path + '/dedud{}_filtered_dedup/'.format(method)\n",
    "        if not os.path.exists(out_folder):\n",
    "            os.mkdir(out_folder)\n",
    "        out_fn = out_folder + out_name\n",
    "        allele_deduplicate_df.to_csv(out_fn, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTCE0BBVY8Df"
   },
   "source": [
    "## Filter recombination edited alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4i82LssY8Df"
   },
   "outputs": [],
   "source": [
    "# Load expected OT sequences\n",
    "sample_df = pd.read_excel('./plasmid_pools_data/NovaSeq3_sample_info.xlsx', sheet_name='Sheet3')\n",
    "amplicon_df = pd.read_csv('./plasmid_pools_data/OT_guide_amplicon_seq.csv', index_col=['OT-Name'])\n",
    "oligo_df = pd.read_excel('./plasmid_pools_data/LVOTUMIv7_untested_ALT_oligos_sgRNAs.xlsx', sheet_name='OT_oligo_designs_ORIGINAL', skiprows=1).rename(columns={'Unnamed: 1':'Barcode'})\n",
    "oligo_df['OT_sequence'] = oligo_df['~3 upstream, ~20 protospacer, 3 PAM, 6 downstream'].str.upper()\n",
    "oligo_df = oligo_df.set_index('OT')[['Barcode', 'OT_sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SMLlbwaY8Df"
   },
   "outputs": [],
   "source": [
    "# Annotate recombination edits\n",
    "for filter in ['dedudS_filtered']:\n",
    "    print(filter)\n",
    "    for fn in tqdm(glob.glob(dir_path + '/{}/*.annot.txt'.format(filter))):\n",
    "        out_name = fn.split('/')[-1]\n",
    "        ot_name = '_'.join(out_name.split('_')[0:4]).replace('_', '-')\n",
    "        barcode = oligo_df.loc[ot_name.replace('-', '_'), 'Barcode']\n",
    "        constant = 'CCAACCTCATAGAACACTCATCC'\n",
    "        target_seq_pos = len(barcode) + len(constant)\n",
    "        allele_df = pd.read_csv(fn, sep='\\t')\n",
    "        # allele_df = allele_df.drop(columns=['best_match_strigent'])\n",
    "        allele_df['best_match'] = ot_name\n",
    "        allele_df['same_match'] = ''\n",
    "        allele_df['#same_match'] = 0\n",
    "        allele_edited = allele_df[allele_df['Unedited'] == False]\n",
    "        recomb_edits_idx = []\n",
    "        mis_targets = []\n",
    "        mis_targets_score = []\n",
    "        same_edits_idx = []\n",
    "        same_targets = []\n",
    "        same_targets_score = []\n",
    "        n_same_targets = []\n",
    "        for idx, row in allele_edited.iterrows():\n",
    "            target_seq = row['Aligned_Sequence'].replace('-', '')\n",
    "            target_seq = target_seq[target_seq_pos:]\n",
    "            expected_dist = levenshteinFullMatrix(target_seq, oligo_df.loc[ot_name.replace('-', '_'), 'OT_sequence'])\n",
    "            current_dist = expected_dist\n",
    "            # print(ot_name, target_seq, oligo_df.loc[ot_name.replace('-', '_'), 'OT_sequence'], current_dist)\n",
    "            best_match = ot_name\n",
    "            same_match = []\n",
    "            # best_match_S = ot_name\n",
    "            for ot, row in oligo_df.iterrows():\n",
    "                ot = ot.replace('_', '-')\n",
    "                if ot == ot_name:\n",
    "                    continue\n",
    "                tdist = levenshteinFullMatrix(target_seq, row['OT_sequence'])\n",
    "                if tdist < current_dist:\n",
    "                    # print('\\t', ot, tdist, target_seq, row['OT_sequence'])\n",
    "                    best_match = ot\n",
    "                    current_dist = tdist\n",
    "                if tdist == expected_dist:\n",
    "                    same_match.append(ot)\n",
    "\n",
    "            if best_match != ot_name:\n",
    "                recomb_edits_idx.append(idx)\n",
    "                mis_targets.append(best_match)\n",
    "                mis_targets_score.append(current_dist)\n",
    "            if len(same_match) > 0:\n",
    "                same_edits_idx.append(idx)\n",
    "                same_targets.append(','.join(same_match))\n",
    "                n_same_targets.append(len(same_match))\n",
    "                same_targets_score.append(expected_dist)\n",
    "                # print(ot_name, best_match)\n",
    "        allele_df.loc[recomb_edits_idx, 'best_match'] = mis_targets\n",
    "        allele_df.loc[same_edits_idx, 'same_match'] = same_targets\n",
    "        allele_df.loc[same_edits_idx, '#same_match'] = n_same_targets\n",
    "        allele_df.loc[recomb_edits_idx, 'best_match_score'] = mis_targets_score\n",
    "        allele_df.loc[same_edits_idx, 'same_match_score'] = same_targets_score\n",
    "        allele_df.to_csv(fn, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MXdeRAIY8Df"
   },
   "outputs": [],
   "source": [
    "# Filter recombination edits\n",
    "for fn in tqdm(glob.glob(dir_path + '/dedudS_filtered/*.annot.txt')):\n",
    "    out_name = fn.split('/')[-1]\n",
    "    allele_df = pd.read_csv(fn, sep='\\t')\n",
    "    allele_df = allele_df[(allele_df['OT_name'] == allele_df['best_match'])&(allele_df['#same_match'] == 0)].reset_index(drop=True)\n",
    "    out_folder = dir_path + '/dedudS_filtered_recomb/'\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.mkdir(out_folder)\n",
    "    out_fn = out_folder + out_name\n",
    "    allele_df.to_csv(out_fn, sep='\\t', index=False)\n",
    "    # allele_df['UMI-Aligned_Sequence'] = allele_df['UMI'] + '-' + allele_df['Aligned_Sequence']\n",
    "    allele_deduplicate_df = deduplicate_umiallele(allele_df)# .drop(columns='level_0')\n",
    "    out_folder = dir_path + '/dedudS_filtered_recomb_dedup/'\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.mkdir(out_folder)\n",
    "    out_fn = out_folder + out_name\n",
    "    allele_deduplicate_df.to_csv(out_fn, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSOJEIzmY8Df"
   },
   "source": [
    "## Edit rate estimation and power analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zpxmljfY8Df"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "foodie_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
