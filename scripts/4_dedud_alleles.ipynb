{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jDB-HAhEY8Da"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import re\n","import gzip\n","from tqdm import tqdm\n","from multiprocessing import Pool\n","import glob\n","import multiprocessing\n","import json\n","import ast"]},{"cell_type":"code","source":["# tar xzvf alleles_HiFi_WTCas9_NoEP.tar.gz"],"metadata":{"id":"NJDBWDIBY9Vu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kk13hBj0Y8Dd"},"outputs":[],"source":["# Functions to process UMI alleles\n","def filter_low_reads_umiallele(df):\n","    filtered_df = df[df['#Reads'] > 1].reset_index()\n","    filtered_df['%Reads'] = filtered_df['#Reads']/filtered_df['#Reads'].sum() * 100\n","    return filtered_df\n","\n","def deduplicate_umiallele(df, filtering=False):\n","    tdf = df.sort_values(by=['UMI', '%Reads_UMI'], ascending=False)\n","    tdf_dedup = tdf.drop_duplicates(subset='UMI', keep='first')\n","    if filtering:\n","        filtered_df = tdf_dedup[tdf_dedup['%Reads_UMI'] > 65].reset_index()\n","    else:\n","        filtered_df = tdf_dedup.reset_index()\n","    filtered_df['#Reads'] = 1\n","    filtered_df['%Reads'] = filtered_df['#Reads']/filtered_df['#Reads'].sum() * 100\n","    return filtered_df\n","\n","def levenshteinFullMatrix(str1, str2):\n","    m = len(str1)\n","    n = len(str2)\n","    # Initialize a matrix to store the edit distances\n","    dp = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n","\n","    # Initialize the first row and column with values from 0 to m and 0 to n respectively\n","    for i in range(m + 1):\n","        dp[i][0] = i\n","    for j in range(n + 1):\n","        dp[0][j] = j\n","\n","    # Fill the matrix using dynamic programming to compute edit distances\n","    for i in range(1, m + 1):\n","        for j in range(1, n + 1):\n","            if str1[i - 1] == str2[j - 1]:\n","                # Characters match, no operation needed\n","                dp[i][j] = dp[i - 1][j - 1]\n","            else:\n","                # Characters don't match, choose minimum cost among insertion, deletion, or substitution\n","                dp[i][j] = 1 + min(dp[i][j - 1], dp[i - 1][j], dp[i - 1][j - 1])\n","\n","    # Return the edit distance between the strings\n","    return dp[m][n]\n","\n","def add_cutsite(fn):\n","    out_name = fn.split('/')[-1]\n","    allele_full_df = pd.read_csv(fn, sep='\\t')\n","    align_20bp = []\n","    ref_20bp = []\n","    for idx, row in allele_full_df.iterrows():\n","        ref_pos = np.array(ast.literal_eval(row['ref_positions']))\n","        cutsite = row['sgRNA_cut_points']\n","        cutsite = np.arange(len(row['Reference_Sequence']))[ref_pos == cutsite][0]\n","        align_20bp.append(row['Aligned_Sequence'][cutsite-9:cutsite+11])\n","        ref_20bp.append(row['Reference_Sequence'][cutsite-9:cutsite+11])\n","    allele_full_df['Unedited'] = allele_full_df['Read_Status'] == 'UNMODIFIED'\n","    allele_full_df['Aligned_Sequence_20bp'] = align_20bp\n","    allele_full_df['Reference_Sequence_20bp'] = ref_20bp\n","    allele_full_df.to_csv(fn, sep='\\t', index=False)\n","    return fn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IkcFryrsY8Dd"},"outputs":[],"source":["# Add cutting site to allele table\n","allele_table_list = []\n","allele_table_dir = './alleles_HiFi_WTCas9_NoEP/'\n","for fn in glob.glob(allele_table_dir+'/*_full_withUMI.txt'):\n","    allele_table_list.append(fn)\n","with Pool(processes=10) as pool:\n","    results = []\n","    for result in tqdm(pool.imap_unordered(add_cutsite, allele_table_list), total=len(allele_table_list)):\n","        results.append(result)"]},{"cell_type":"markdown","metadata":{"id":"nwlE0Hf2Y8De"},"source":["## Load UMI annotation from plasmid pools"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AW8Q2WhEY8De"},"outputs":[],"source":["maxi_ds_df = pd.read_csv('./plasmid_pools_data/trimmed_LV-Maxi-merged3_L001_R1_001.fastq.gz.extractedSeq.UMI-Barcode-dedup-dedup.annot.csv')\n","maxi_ds_df['good_L0'] = maxi_ds_df['with_expected_OT']\n","maxi_ds_df['good_S'] = (maxi_ds_df['#UMI-Barcode'] == maxi_ds_df['#UMI-Seq-merged'])&(maxi_ds_df['with_expected_OT'])\n","umi_complexity = pd.read_csv('./plasmid_pools_data/UMI_entropy_barcode_count_merged_plasmid_pools.csv', index_col='umi_r1')"]},{"cell_type":"markdown","metadata":{"id":"8lEO3It1Y8De"},"source":["## Annotate Raw allele based on Barcode annotation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TeBlKHCGY8De"},"outputs":[],"source":["os.makedirs('./target_umi_barcode_table_WTCas9_NoEP', exist_ok=True)\n","out_folder = './target_umi_barcode_table_WTCas9_NoEP/raw/'\n","os.makedirs(out_folder, exist_ok=True)\n","# allele_table_dir = './alleles_HiFi_WTCas9_NoEP/'\n","for fn in tqdm(glob.glob(allele_table_dir + '/*_full_withUMI.txt')):\n","    ot_name = fn.split('/')[-1].split('_')[0]\n","    if ot_name not in sel_OTs:\n","        continue\n","    out_name = fn.split('/')[-1]\n","    allele_df = pd.read_csv(fn, sep='\\t')[['UMI', 'index', 'Aligned_Sequence',\n","               'Reference_Sequence', 'Aligned_Sequence_20bp', 'Reference_Sequence_20bp', 'sgRNA_cut_points', 'Aligned_Reference_Scores', 'Unedited', 'n_deleted', 'n_inserted',\n","               'n_mutated', '#Reads', '%Reads_UMI', '%Reads', 'sample_id', 'donor',\n","               'replicate', 'group', 'OT_name']]\n","    ot_name = allele_df.iloc[0]['OT_name'].replace('-', '_')\n","    OT_maxi_df = maxi_ds_df[(maxi_ds_df['OT'] == ot_name)].drop_duplicates(subset='umi_r1', keep='first')\n","    umi_set = list(set(allele_df['UMI'].unique()).intersection(umi_complexity.index))\n","    allele_df = allele_df.merge(umi_complexity.loc[umi_set][['#barcode', 'entropy']], left_on='UMI', right_index=True, how='left')\n","    allele_maxi_df = allele_df.merge(OT_maxi_df, how='left', left_on='UMI', right_on='umi_r1')\n","    allele_maxi_df.loc[allele_maxi_df[allele_maxi_df['good_L0'].isna()].index, 'good_L0'] = 'Unseen'\n","    allele_maxi_df.loc[allele_maxi_df[allele_maxi_df['with_expected_OT'].isna()].index, 'with_expected_OT'] = 'Unseen'\n","    allele_maxi_df['good_L0'] = allele_maxi_df['good_L0'].astype(str)\n","    # Stringent\n","    allele_maxi_df.loc[allele_maxi_df[allele_maxi_df['good_S'].isna()].index, 'good_S'] = 'Unseen'\n","    allele_maxi_df.loc[allele_maxi_df[allele_maxi_df['with_expected_OT'].isna()].index, 'with_expected_OT'] = 'Unseen'\n","    allele_maxi_df['good_S'] = allele_maxi_df['good_S'].astype(str)\n","    allele_maxi_df['with_expected_OT'] = allele_maxi_df['with_expected_OT'].astype(str)\n","    allele_maxi_df.to_csv(out_folder + '/' + out_name.replace('.txt', '.annot.txt'), index=False, sep='\\t')"]},{"cell_type":"markdown","metadata":{"id":"xkKawg0hY8De"},"source":["## Filter alleles based on UMI annotetion (Dedud, Dedup and Dedud+dedup)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kV_q6dmlY8De"},"outputs":[],"source":["dir_path = './target_umi_barcode_table_WTCas9_NoEP/' # output folder\n","for fn in tqdm(glob.glob(dir_path +'/raw/*.annot.txt')):\n","    out_name = fn.split('/')[-1]\n","    allele_df = pd.read_csv(fn, sep='\\t')\n","    # allele_df['UMI-Aligned_Sequence'] = allele_df['UMI'] + '-' + allele_df['Aligned_Sequence']\n","    donor = allele_df.iloc[0]['donor']\n","    edit = allele_df.iloc[0]['group']\n","    allele_deduplicate_df = deduplicate_umiallele(allele_df)# .drop(columns='level_0')\n","    out_folder = '{}/raw_dedup/'.format(dir_path)\n","    if not os.path.exists(out_folder):\n","        os.mkdir(out_folder)\n","    out_fn = out_folder + out_name\n","    allele_deduplicate_df.to_csv(out_fn, sep='\\t', index=False)\n","\n","    for method in ['S']:\n","        umi_type_name = 'good_' + method\n","        dedud_df = allele_df[allele_df[umi_type_name] == 'True'].reset_index()\n","        dedud_df['%Reads'] = dedud_df['#Reads']/dedud_df['#Reads'].sum() * 100\n","        dedud_df = dedud_df.drop(columns='level_0')\n","        out_folder = dir_path + '/dedud{}/'.format(method)\n","        if not os.path.exists(out_folder):\n","            os.mkdir(out_folder)\n","        dedud_df.to_csv(out_folder + out_name, sep='\\t', index=False)\n","\n","        # allele_deduplicate_df = deduplicate_umiallele(dedud_df).drop(columns='level_0')\n","        # out_folder = './target_umi_barcode_table_full/dedud{}_dedup/'.format(method)\n","        # if not os.path.exists(out_folder):\n","        #     os.mkdir(out_folder)\n","        # out_fn = out_folder + out_name\n","        # allele_deduplicate_df.to_csv(out_fn, sep='\\t')\n","\n","        dedud_filtered_df = dedud_df[(dedud_df['entropy'] > 1.477)&(dedud_df['#barcode'] == 1)|(dedud_df['#barcode'] == 2)]\n","        out_folder = dir_path + '/dedud{}_filtered/'.format(method)\n","        if not os.path.exists(out_folder):\n","            os.mkdir(out_folder)\n","        dedud_filtered_df.to_csv(out_folder + out_name, sep='\\t', index=False)\n","\n","        allele_deduplicate_df = deduplicate_umiallele(dedud_filtered_df).drop(columns='level_0')\n","        out_folder = dir_path + '/dedud{}_filtered_dedup/'.format(method)\n","        if not os.path.exists(out_folder):\n","            os.mkdir(out_folder)\n","        out_fn = out_folder + out_name\n","        allele_deduplicate_df.to_csv(out_fn, sep='\\t')"]},{"cell_type":"markdown","metadata":{"id":"QTCE0BBVY8Df"},"source":["## Filter recombination edited alleles"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L4i82LssY8Df"},"outputs":[],"source":["# Load expected OT sequences\n","sample_df = pd.read_excel('./plasmid_pools_data/NovaSeq3_sample_info.xlsx', sheet_name='Sheet3')\n","amplicon_df = pd.read_csv('./plasmid_pools_data/OT_guide_amplicon_seq.csv', index_col=['OT-Name'])\n","oligo_df = pd.read_excel('./plasmid_pools_data/LVOTUMIv7_untested_ALT_oligos_sgRNAs.xlsx', sheet_name='OT_oligo_designs_ORIGINAL', skiprows=1).rename(columns={'Unnamed: 1':'Barcode'})\n","oligo_df['OT_sequence'] = oligo_df['~3 upstream, ~20 protospacer, 3 PAM, 6 downstream'].str.upper()\n","oligo_df = oligo_df.set_index('OT')[['Barcode', 'OT_sequence']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_SMLlbwaY8Df"},"outputs":[],"source":["# Annotate recombination edits\n","for filter in ['dedudS_filtered']:\n","    print(filter)\n","    for fn in tqdm(glob.glob(dir_path + '/{}/*.annot.txt'.format(filter))):\n","        out_name = fn.split('/')[-1]\n","        ot_name = '_'.join(out_name.split('_')[0:4]).replace('_', '-')\n","        barcode = oligo_df.loc[ot_name.replace('-', '_'), 'Barcode']\n","        constant = 'CCAACCTCATAGAACACTCATCC'\n","        target_seq_pos = len(barcode) + len(constant)\n","        allele_df = pd.read_csv(fn, sep='\\t')\n","        # allele_df = allele_df.drop(columns=['best_match_strigent'])\n","        allele_df['best_match'] = ot_name\n","        allele_df['same_match'] = ''\n","        allele_df['#same_match'] = 0\n","        allele_edited = allele_df[allele_df['Unedited'] == False]\n","        recomb_edits_idx = []\n","        mis_targets = []\n","        mis_targets_score = []\n","        same_edits_idx = []\n","        same_targets = []\n","        same_targets_score = []\n","        n_same_targets = []\n","        for idx, row in allele_edited.iterrows():\n","            target_seq = row['Aligned_Sequence'].replace('-', '')\n","            target_seq = target_seq[target_seq_pos:]\n","            expected_dist = levenshteinFullMatrix(target_seq, oligo_df.loc[ot_name.replace('-', '_'), 'OT_sequence'])\n","            current_dist = expected_dist\n","            # print(ot_name, target_seq, oligo_df.loc[ot_name.replace('-', '_'), 'OT_sequence'], current_dist)\n","            best_match = ot_name\n","            same_match = []\n","            # best_match_S = ot_name\n","            for ot, row in oligo_df.iterrows():\n","                ot = ot.replace('_', '-')\n","                if ot == ot_name:\n","                    continue\n","                tdist = levenshteinFullMatrix(target_seq, row['OT_sequence'])\n","                if tdist < current_dist:\n","                    # print('\\t', ot, tdist, target_seq, row['OT_sequence'])\n","                    best_match = ot\n","                    current_dist = tdist\n","                if tdist == expected_dist:\n","                    same_match.append(ot)\n","\n","            if best_match != ot_name:\n","                recomb_edits_idx.append(idx)\n","                mis_targets.append(best_match)\n","                mis_targets_score.append(current_dist)\n","            if len(same_match) > 0:\n","                same_edits_idx.append(idx)\n","                same_targets.append(','.join(same_match))\n","                n_same_targets.append(len(same_match))\n","                same_targets_score.append(expected_dist)\n","                # print(ot_name, best_match)\n","        allele_df.loc[recomb_edits_idx, 'best_match'] = mis_targets\n","        allele_df.loc[same_edits_idx, 'same_match'] = same_targets\n","        allele_df.loc[same_edits_idx, '#same_match'] = n_same_targets\n","        allele_df.loc[recomb_edits_idx, 'best_match_score'] = mis_targets_score\n","        allele_df.loc[same_edits_idx, 'same_match_score'] = same_targets_score\n","        allele_df.to_csv(fn, sep='\\t', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4MXdeRAIY8Df"},"outputs":[],"source":["# Filter recombination edits\n","for fn in tqdm(glob.glob(dir_path + '/dedudS_filtered/*.annot.txt')):\n","    out_name = fn.split('/')[-1]\n","    allele_df = pd.read_csv(fn, sep='\\t')\n","    allele_df = allele_df[(allele_df['OT_name'] == allele_df['best_match'])&(allele_df['#same_match'] == 0)].reset_index(drop=True)\n","    out_folder = dir_path + '/dedudS_filtered_recomb/'\n","    if not os.path.exists(out_folder):\n","        os.mkdir(out_folder)\n","    out_fn = out_folder + out_name\n","    allele_df.to_csv(out_fn, sep='\\t', index=False)\n","    # allele_df['UMI-Aligned_Sequence'] = allele_df['UMI'] + '-' + allele_df['Aligned_Sequence']\n","    allele_deduplicate_df = deduplicate_umiallele(allele_df)# .drop(columns='level_0')\n","    out_folder = dir_path + '/dedudS_filtered_recomb_dedup/'\n","    if not os.path.exists(out_folder):\n","        os.mkdir(out_folder)\n","    out_fn = out_folder + out_name\n","    allele_deduplicate_df.to_csv(out_fn, sep='\\t', index=False)"]},{"cell_type":"markdown","metadata":{"id":"kSOJEIzmY8Df"},"source":["## Edit rate estimation and power analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6zpxmljfY8Df"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"jc_py311","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}